{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_steps = 10000\n",
    "batch_size = 128\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 28\n",
    "num_classes = 10\n",
    "timesteps =28\n",
    "num_hidden = 128\n",
    "\n",
    "X = tf.placeholder(\"float\", [None,timesteps,num_input])\n",
    "Y = tf.placeholder(\"float\", [None,num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Weights\n",
    "weights ={\n",
    "    'out':tf.Variable(tf.random_normal([num_hidden,num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Input shape\n",
    "- Samples - This is the len(dataX), or the amount of data points you have. Or individual training examples\n",
    "\n",
    "- Time steps - This is equivalent to the amount of time steps you run your recurrent neural network. If you want your network to have memory of 60 characters, this number should be 60.\n",
    "\n",
    "- Features - this is the amount of features in every time step. If you are processing pictures, this is the amount of pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, n_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0, reuse=True)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.static_rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Functions used \n",
    " - Logits is a function which operates on the unscaled output of earlier layers and on a linear scale to understand the linear units. In Mathematics, Logits is a function that maps probabilities ( [0, 1] ) from real values R ( (-inf, inf) ) .\n",
    " - Softmax function turns logits [2.0, 1.0, 0.1] into probabilities [0.7, 0.2, 0.1], and the probabilities sum to 1.\n",
    " - cross entropy: \n",
    " H(p,q) = p(x)*log(q(x))\n",
    " \n",
    " Where p(x) is the true probability of event x and q(x) is the predicted probability of event x.\n",
    "\n",
    "There if input any two numbers for p(x) and q(x) are used such that\n",
    "\n",
    "0<p(x)<1 AND 0<q(x)<1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = RNN(X, weights, biases)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Minibatch Loss= 3.5185, Training Accuracy= 0.070\n",
      "Step 200, Minibatch Loss= 2.1969, Training Accuracy= 0.273\n",
      "Step 400, Minibatch Loss= 1.9854, Training Accuracy= 0.391\n",
      "Step 600, Minibatch Loss= 1.8148, Training Accuracy= 0.398\n",
      "Step 800, Minibatch Loss= 1.5902, Training Accuracy= 0.531\n",
      "Step 1000, Minibatch Loss= 1.7106, Training Accuracy= 0.445\n",
      "Step 1200, Minibatch Loss= 1.4515, Training Accuracy= 0.547\n",
      "Step 1400, Minibatch Loss= 1.4485, Training Accuracy= 0.531\n",
      "Step 1600, Minibatch Loss= 1.3677, Training Accuracy= 0.594\n",
      "Step 1800, Minibatch Loss= 1.3501, Training Accuracy= 0.508\n",
      "Step 2000, Minibatch Loss= 1.3537, Training Accuracy= 0.594\n",
      "Step 2200, Minibatch Loss= 1.3762, Training Accuracy= 0.516\n",
      "Step 2400, Minibatch Loss= 1.2363, Training Accuracy= 0.602\n",
      "Step 2600, Minibatch Loss= 1.1887, Training Accuracy= 0.656\n",
      "Step 2800, Minibatch Loss= 1.0488, Training Accuracy= 0.672\n",
      "Step 3000, Minibatch Loss= 1.1559, Training Accuracy= 0.648\n",
      "Step 3200, Minibatch Loss= 1.1616, Training Accuracy= 0.641\n",
      "Step 3400, Minibatch Loss= 1.1595, Training Accuracy= 0.594\n",
      "Step 3600, Minibatch Loss= 0.8421, Training Accuracy= 0.719\n",
      "Step 3800, Minibatch Loss= 0.8966, Training Accuracy= 0.734\n",
      "Step 4000, Minibatch Loss= 0.9735, Training Accuracy= 0.672\n",
      "Step 4200, Minibatch Loss= 0.7838, Training Accuracy= 0.758\n",
      "Step 4400, Minibatch Loss= 0.8711, Training Accuracy= 0.742\n",
      "Step 4600, Minibatch Loss= 0.8257, Training Accuracy= 0.727\n",
      "Step 4800, Minibatch Loss= 0.8630, Training Accuracy= 0.703\n",
      "Step 5000, Minibatch Loss= 0.8686, Training Accuracy= 0.742\n",
      "Step 5200, Minibatch Loss= 0.8507, Training Accuracy= 0.727\n",
      "Step 5400, Minibatch Loss= 0.6828, Training Accuracy= 0.766\n",
      "Step 5600, Minibatch Loss= 0.7437, Training Accuracy= 0.789\n",
      "Step 5800, Minibatch Loss= 0.7673, Training Accuracy= 0.766\n",
      "Step 6000, Minibatch Loss= 0.7119, Training Accuracy= 0.805\n",
      "Step 6200, Minibatch Loss= 0.7662, Training Accuracy= 0.727\n",
      "Step 6400, Minibatch Loss= 0.8397, Training Accuracy= 0.727\n",
      "Step 6600, Minibatch Loss= 0.7160, Training Accuracy= 0.797\n",
      "Step 6800, Minibatch Loss= 0.6551, Training Accuracy= 0.766\n",
      "Step 7000, Minibatch Loss= 0.5647, Training Accuracy= 0.828\n",
      "Step 7200, Minibatch Loss= 0.6548, Training Accuracy= 0.828\n",
      "Step 7400, Minibatch Loss= 0.5670, Training Accuracy= 0.875\n",
      "Step 7600, Minibatch Loss= 0.5888, Training Accuracy= 0.820\n",
      "Step 7800, Minibatch Loss= 0.4539, Training Accuracy= 0.836\n",
      "Step 8000, Minibatch Loss= 0.4293, Training Accuracy= 0.852\n",
      "Step 8200, Minibatch Loss= 0.5673, Training Accuracy= 0.859\n",
      "Step 8400, Minibatch Loss= 0.6332, Training Accuracy= 0.789\n",
      "Step 8600, Minibatch Loss= 0.5747, Training Accuracy= 0.836\n",
      "Step 8800, Minibatch Loss= 0.6112, Training Accuracy= 0.766\n",
      "Step 9000, Minibatch Loss= 0.6510, Training Accuracy= 0.781\n",
      "Step 9200, Minibatch Loss= 0.6201, Training Accuracy= 0.859\n",
      "Step 9400, Minibatch Loss= 0.5042, Training Accuracy= 0.812\n",
      "Step 9600, Minibatch Loss= 0.5934, Training Accuracy= 0.820\n",
      "Step 9800, Minibatch Loss= 0.5215, Training Accuracy= 0.844\n",
      "Step 10000, Minibatch Loss= 0.5046, Training Accuracy= 0.820\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.8671875\n"
     ]
    }
   ],
   "source": [
    "# start Training\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    \n",
    "    for step in range(1,training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape the data to get 28x28 elements\n",
    "        batch_x = batch_x.reshape((batch_size,timesteps,num_input))\n",
    "        #Run the optimization\n",
    "        sess.run(train_op,feed_dict={X:batch_x, Y:batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x, Y: batch_y})  \n",
    "            \n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    \n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type help() for interactive help, or help(object) for help about object."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
